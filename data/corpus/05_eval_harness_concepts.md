# Evaluation Harness Concepts

The evaluation harness runs a dataset of questions and records results per model. It tracks simple correctness proxies, refusal correctness, and latency.

Even a lightweight benchmark is useful because it exposes regressions when prompts, chunking, or retrieval settings change.

