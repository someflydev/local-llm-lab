{"instruction": "Give a quick tip for choosing a chat model in this lab.", "response": "Use lab models recommend --task chat and start with llama3 if it is installed.", "id": "prompting_001"}
{"instruction": "Describe the purpose of lab ingest in one sentence.", "response": "lab ingest builds a local sqlite retrieval index from the corpus using the selected embeddings model.", "id": "prompting_002"}
{"instruction": "How should the system behave when RAG evidence is missing?", "response": "It should refuse and return the exact configured refusal string instead of guessing.", "id": "prompting_003"}
{"instruction": "What is one practical performance knob on laptop hardware?", "response": "Reduce num_ctx or retrieved chunks to improve responsiveness and reduce memory pressure.", "id": "prompting_004"}
{"instruction": "How should operators manage local model storage?", "response": "Pull only needed models and remove unused ones to protect limited local storage.", "id": "prompting_005"}
{"instruction": "Explain why lab doctor should be run first.", "response": "Run lab doctor first to verify Ollama connectivity, installed models, and local environment health before other commands.", "id": "prompting_006"}
{"instruction": "Give a quick tip for choosing a chat model in this lab.", "response": "Use lab models recommend --task chat and start with llama3 if it is installed.", "id": "prompting_007"}
{"instruction": "Describe the purpose of lab ingest in one sentence.", "response": "lab ingest builds a local sqlite retrieval index from the corpus using the selected embeddings model.", "id": "prompting_008"}
{"instruction": "How should the system behave when RAG evidence is missing?", "response": "It should refuse and return the exact configured refusal string instead of guessing.", "id": "prompting_009"}
{"instruction": "What is one practical performance knob on laptop hardware?", "response": "Reduce num_ctx or retrieved chunks to improve responsiveness and reduce memory pressure.", "id": "prompting_010"}
{"instruction": "How should operators manage local model storage?", "response": "Pull only needed models and remove unused ones to protect limited local storage.", "id": "prompting_011"}
{"instruction": "Explain why lab doctor should be run first.", "response": "Run lab doctor first to verify Ollama connectivity, installed models, and local environment health before other commands.", "id": "prompting_012"}
{"instruction": "Give a quick tip for choosing a chat model in this lab.", "response": "Use lab models recommend --task chat and start with llama3 if it is installed.", "id": "prompting_013"}
{"instruction": "Describe the purpose of lab ingest in one sentence.", "response": "lab ingest builds a local sqlite retrieval index from the corpus using the selected embeddings model.", "id": "prompting_014"}
{"instruction": "How should the system behave when RAG evidence is missing?", "response": "It should refuse and return the exact configured refusal string instead of guessing.", "id": "prompting_015"}
{"instruction": "What is one practical performance knob on laptop hardware?", "response": "Reduce num_ctx or retrieved chunks to improve responsiveness and reduce memory pressure.", "id": "prompting_016"}
{"instruction": "How should operators manage local model storage?", "response": "Pull only needed models and remove unused ones to protect limited local storage.", "id": "prompting_017"}
{"instruction": "Explain why lab doctor should be run first.", "response": "Run lab doctor first to verify Ollama connectivity, installed models, and local environment health before other commands.", "id": "prompting_018"}
{"instruction": "Give a quick tip for choosing a chat model in this lab.", "response": "Use lab models recommend --task chat and start with llama3 if it is installed.", "id": "prompting_019"}
{"instruction": "Describe the purpose of lab ingest in one sentence.", "response": "lab ingest builds a local sqlite retrieval index from the corpus using the selected embeddings model.", "id": "prompting_020"}
{"instruction": "How should the system behave when RAG evidence is missing?", "response": "It should refuse and return the exact configured refusal string instead of guessing.", "id": "prompting_021"}
{"instruction": "What is one practical performance knob on laptop hardware?", "response": "Reduce num_ctx or retrieved chunks to improve responsiveness and reduce memory pressure.", "id": "prompting_022"}
{"instruction": "How should operators manage local model storage?", "response": "Pull only needed models and remove unused ones to protect limited local storage.", "id": "prompting_023"}
{"instruction": "Explain why lab doctor should be run first.", "response": "Run lab doctor first to verify Ollama connectivity, installed models, and local environment health before other commands.", "id": "prompting_024"}
{"instruction": "Give a quick tip for choosing a chat model in this lab.", "response": "Use lab models recommend --task chat and start with llama3 if it is installed.", "id": "prompting_025"}
{"instruction": "Describe the purpose of lab ingest in one sentence.", "response": "lab ingest builds a local sqlite retrieval index from the corpus using the selected embeddings model.", "id": "prompting_026"}
{"instruction": "How should the system behave when RAG evidence is missing?", "response": "It should refuse and return the exact configured refusal string instead of guessing.", "id": "prompting_027"}
{"instruction": "What is one practical performance knob on laptop hardware?", "response": "Reduce num_ctx or retrieved chunks to improve responsiveness and reduce memory pressure.", "id": "prompting_028"}
{"instruction": "How should operators manage local model storage?", "response": "Pull only needed models and remove unused ones to protect limited local storage.", "id": "prompting_029"}
{"instruction": "Explain why lab doctor should be run first.", "response": "Run lab doctor first to verify Ollama connectivity, installed models, and local environment health before other commands.", "id": "prompting_030"}
{"instruction": "Give a quick tip for choosing a chat model in this lab.", "response": "Use lab models recommend --task chat and start with llama3 if it is installed.", "id": "prompting_031"}
{"instruction": "Describe the purpose of lab ingest in one sentence.", "response": "lab ingest builds a local sqlite retrieval index from the corpus using the selected embeddings model.", "id": "prompting_032"}
{"instruction": "How should the system behave when RAG evidence is missing?", "response": "It should refuse and return the exact configured refusal string instead of guessing.", "id": "prompting_033"}
{"instruction": "What is one practical performance knob on laptop hardware?", "response": "Reduce num_ctx or retrieved chunks to improve responsiveness and reduce memory pressure.", "id": "prompting_034"}
{"instruction": "How should operators manage local model storage?", "response": "Pull only needed models and remove unused ones to protect limited local storage.", "id": "prompting_035"}
{"instruction": "Explain why lab doctor should be run first.", "response": "Run lab doctor first to verify Ollama connectivity, installed models, and local environment health before other commands.", "id": "prompting_036"}
{"instruction": "Give a quick tip for choosing a chat model in this lab.", "response": "Use lab models recommend --task chat and start with llama3 if it is installed.", "id": "prompting_037"}
{"instruction": "Describe the purpose of lab ingest in one sentence.", "response": "lab ingest builds a local sqlite retrieval index from the corpus using the selected embeddings model.", "id": "prompting_038"}
{"instruction": "How should the system behave when RAG evidence is missing?", "response": "It should refuse and return the exact configured refusal string instead of guessing.", "id": "prompting_039"}
{"instruction": "What is one practical performance knob on laptop hardware?", "response": "Reduce num_ctx or retrieved chunks to improve responsiveness and reduce memory pressure.", "id": "prompting_040"}
{"instruction": "How should operators manage local model storage?", "response": "Pull only needed models and remove unused ones to protect limited local storage.", "id": "prompting_041"}
{"instruction": "Explain why lab doctor should be run first.", "response": "Run lab doctor first to verify Ollama connectivity, installed models, and local environment health before other commands.", "id": "prompting_042"}
{"instruction": "Give a quick tip for choosing a chat model in this lab.", "response": "Use lab models recommend --task chat and start with llama3 if it is installed.", "id": "prompting_043"}
{"instruction": "Describe the purpose of lab ingest in one sentence.", "response": "lab ingest builds a local sqlite retrieval index from the corpus using the selected embeddings model.", "id": "prompting_044"}
{"instruction": "How should the system behave when RAG evidence is missing?", "response": "It should refuse and return the exact configured refusal string instead of guessing.", "id": "prompting_045"}
{"instruction": "What is one practical performance knob on laptop hardware?", "response": "Reduce num_ctx or retrieved chunks to improve responsiveness and reduce memory pressure.", "id": "prompting_046"}
{"instruction": "How should operators manage local model storage?", "response": "Pull only needed models and remove unused ones to protect limited local storage.", "id": "prompting_047"}
{"instruction": "Explain why lab doctor should be run first.", "response": "Run lab doctor first to verify Ollama connectivity, installed models, and local environment health before other commands.", "id": "prompting_048"}
{"instruction": "Give a quick tip for choosing a chat model in this lab.", "response": "Use lab models recommend --task chat and start with llama3 if it is installed.", "id": "prompting_049"}
{"instruction": "Describe the purpose of lab ingest in one sentence.", "response": "lab ingest builds a local sqlite retrieval index from the corpus using the selected embeddings model.", "id": "prompting_050"}
{"instruction": "How should the system behave when RAG evidence is missing?", "response": "It should refuse and return the exact configured refusal string instead of guessing.", "id": "prompting_051"}
{"instruction": "What is one practical performance knob on laptop hardware?", "response": "Reduce num_ctx or retrieved chunks to improve responsiveness and reduce memory pressure.", "id": "prompting_052"}
{"instruction": "How should operators manage local model storage?", "response": "Pull only needed models and remove unused ones to protect limited local storage.", "id": "prompting_053"}
{"instruction": "Explain why lab doctor should be run first.", "response": "Run lab doctor first to verify Ollama connectivity, installed models, and local environment health before other commands.", "id": "prompting_054"}
{"instruction": "Give a quick tip for choosing a chat model in this lab.", "response": "Use lab models recommend --task chat and start with llama3 if it is installed.", "id": "prompting_055"}
{"instruction": "Describe the purpose of lab ingest in one sentence.", "response": "lab ingest builds a local sqlite retrieval index from the corpus using the selected embeddings model.", "id": "prompting_056"}
{"instruction": "How should the system behave when RAG evidence is missing?", "response": "It should refuse and return the exact configured refusal string instead of guessing.", "id": "prompting_057"}
{"instruction": "What is one practical performance knob on laptop hardware?", "response": "Reduce num_ctx or retrieved chunks to improve responsiveness and reduce memory pressure.", "id": "prompting_058"}
{"instruction": "How should operators manage local model storage?", "response": "Pull only needed models and remove unused ones to protect limited local storage.", "id": "prompting_059"}
{"instruction": "Explain why lab doctor should be run first.", "response": "Run lab doctor first to verify Ollama connectivity, installed models, and local environment health before other commands.", "id": "prompting_060"}
